{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ee8df5",
   "metadata": {},
   "source": [
    "# Analyzing Climate Talk\n",
    "## Studying the Impact of Rhetorical Strategies in Environmentally-Related Discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb677f",
   "metadata": {},
   "source": [
    "In the complex fight against climate change, effective communication is key. Understanding how different rhetoric is used to shape public perception and spur action can make all the difference. I want to find which types of language are most indicative of propaganda versus scientifically backed information to gauge their influence on public understanding of climate change.\n",
    "\n",
    "From understanding the ways propaganda and scientifically backed works communicate their messages, we can learn how to better inform and engage the public. Analyzing the language used in these different types of rhetoric will help us understand the divides that influence how climate change is perceived and acted upon. My research intends to break these divisions and instead promote a more informed and unified approach to understanding environmental issues.\n",
    "\n",
    "By not only addressing an urgent global challenge but also using the power of digital humanities to find the intricate ways in which language shapes our world, my study aims to contribute to more effective climate communication strategies for greater public awareness and action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd4955",
   "metadata": {},
   "source": [
    "The primary motivation for pursuing this research question came from completing Assignment 05, where we learned how to use Natural Language Processing to create strong data visualizations. I wanted to apply these tools to a new dataset on a topic I feel deeply about. I believe that understanding the severity of global warming is crucial, and one of the issues is how we discuss it. The language we use often fails to convey the urgency of the situation, and I want to explore how this affects public perception and potential solutions.\n",
    "\n",
    "All my data will be compiled from the online Climate section of [The Atlantic](https://www.theatlantic.com/), because after testing many reputable publications, I found that it has fewer restrictions on web scraping. I will use Python packages specializing in web scraping, such as Beautiful Soup and Requests, to parse HTML documents and extract links to various articles. Once I have access to these articles, I will use Pandas to organize the data into an analyzable format. For text processing, I will utilize NLTK, and for creating visualizations, I will use Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7e334",
   "metadata": {},
   "source": [
    "### DATA COLLECTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e833ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa440f6d",
   "metadata": {},
   "source": [
    "The first step in our analysis was to set up the web scraping process. To retrieve data from the web, I imported two libraries, `requests` to retrieve the HTML code from The Atlantic's [Climate](https://www.theatlantic.com/category/climate/) webpage and `BeautifulSoup` to parse the HTML and find links to various articles on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be89bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.theatlantic.com/category/climate/')\n",
    "html_string = response.text\n",
    "\n",
    "doc = BeautifulSoup(html_string, 'html.parser')\n",
    "article_list = []\n",
    "    \n",
    "for li in doc.select('ul li a'):\n",
    "        href = li.get('href')\n",
    "        if href and '/archive' in href:\n",
    "            article_list.append(href)\n",
    "          \n",
    "for a in doc.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    if '/archive' in href:\n",
    "        article_list.append(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc03be",
   "metadata": {},
   "source": [
    "By searching for all occurrences of anchor tags within lists and also all anchor tags on the page, we created a variable `article_list` that contains a list of links to the latest Atlantic articles on climate. To avoid irrelevant links to images and other navigation, we filtered specifically for links that contain `/archive` in their `href` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97333010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleText(article):\n",
    "  response = requests.get(article)\n",
    "  doc = BeautifulSoup(response.text, 'html.parser')\n",
    "  \n",
    "  article_body = doc.find_all(class_='ArticleParagraph_root__4mszW')\n",
    "  article_text = ''\n",
    "  \n",
    "  for paragraph in article_body:\n",
    "    article_text += paragraph.get_text() + '\\n'\n",
    "    \n",
    "  return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec8f26",
   "metadata": {},
   "source": [
    "I have defined a function `getArticleText` to open a particular url and return its paragraph text in a string form without unnecessary headers or links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d46c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sampled_articles = random.sample(article_list, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c857c3",
   "metadata": {},
   "source": [
    "Because [The Atlantic](https://www.theatlantic.com/) has an abundance of written works, I imported Python's `random` module to take a random sample of 6 article URLs from the list to control my data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a9bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articleDF = pd.DataFrame(columns=['url', 'text'])\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for article_url in sampled_articles:\n",
    "  if article_url.startswith('/'):\n",
    "    article_url = 'https://www.theatlantic.com' + article_url\n",
    "  article_text = getArticleText(article_url)\n",
    "    \n",
    "  df_list.append(pd.DataFrame({'url': [article_url], 'text': [article_text]}))\n",
    "\n",
    "articleDF = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786abf2",
   "metadata": {},
   "source": [
    "I have now imported the `pandas` library to set up for data manipulation and analysis. I then set up a for-loop that retrieved the text using the `getArticleText` function defined earlier. To organize all the data and prevent overwriting variables with each iteration, I created a list to store individual data frames for each article's `article_text` and `article_url`. After collecting all the data frames, I concatenated them into a single data frame to store the information for all articles in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dddc5",
   "metadata": {},
   "source": [
    "### TEXT PROCESSING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c315c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import vader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fdbe5",
   "metadata": {},
   "source": [
    "To begin the data exploration portion of my final project, I have imported the Natural Language Toolkit (`NLTK`) and its various modules to help with starting a more focused analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e897b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = (stopwords.words('english'))\n",
    "senti = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "def processText(text):\n",
    "  tokenized_text = word_tokenize(text)\n",
    "  noStop_text = [word for word in tokenized_text if word.lower() not in stop_words]\n",
    "  \n",
    "  stemmed_text = [PorterStemmer().stem(word) for word in noStop_text]\n",
    "  return stemmed_text\n",
    "\n",
    "articleDF['processed'] = articleDF['text'].apply(processText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1f342",
   "metadata": {},
   "source": [
    "Above, I began by creating two variables to help me with syntax in using NLTK modules. I then defined a function to process the raw text from [The Atlantic](https://www.theatlantic.com/) articles by tokenizing the string, removing stop words, and words with the same stem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde6e8a",
   "metadata": {},
   "source": [
    "### ANALYSIS + VISUALIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289e7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# compares total frequency of positive and negative words\n",
    "def PosNeg(text):\n",
    "  pos_count = 0\n",
    "  neg_count = 0\n",
    "\n",
    "  for word in text:\n",
    "    score = senti.polarity_scores(word)\n",
    "    if score['compound'] > 0:\n",
    "      pos_count += 1\n",
    "    elif score['compound'] < 0:\n",
    "      neg_count += 1\n",
    "\n",
    "  return pos_count, neg_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec001e",
   "metadata": {},
   "source": [
    "The `PosNeg` function takes in a given text and calculates the total number of positive and negative words by using the VADER sentiment analyzer to score each word, incrementing variables `pos_count` for positive scores and `neg_count` for negative scores. The function returns the counts of positive and negative words, visualizing information into the general sentiment of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dbd3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compares 15 most common positive and negative words\n",
    "def PosNegFreq(text):\n",
    "  pos_words = []\n",
    "  neg_words = []\n",
    "  \n",
    "  for word in text:\n",
    "    score = senti.polarity_scores(word)\n",
    "    if score['compound'] > 0:\n",
    "      pos_words.append(word)\n",
    "    elif score['compound'] < 0:\n",
    "      neg_words.append(word)\n",
    "\n",
    "  pos_common = nltk.FreqDist(pos_words).most_common(15)\n",
    "  neg_common = nltk.FreqDist(neg_words).most_common(15)\n",
    "  \n",
    "  pos = []\n",
    "  pos_freq = []\n",
    "  for i in pos_common:\n",
    "    pos.append(i[0])\n",
    "    pos_freq.append(i[1])\n",
    "    \n",
    "  neg = []\n",
    "  neg_freq = []\n",
    "  for i in neg_common:\n",
    "    neg.append(i[0])\n",
    "    neg_freq.append(i[1])\n",
    "    \n",
    "  return pos, pos_freq, neg, neg_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bcf40",
   "metadata": {},
   "source": [
    "The `PosNegFreq` function identifies the 15 most common positive and negative words in a given text. It classifies words based on their sentiment scores using the VADER analyzer, and then calculates their frequencies, returning four lists: the most common positive words, their frequencies, the most common negative words, and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a6174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankByNeg(text):\n",
    "  neg_score = []\n",
    "  \n",
    "  for index, row in text.iterrows():\n",
    "    text = processText(row['text'])\n",
    "    pos_count, neg_count = PosNeg(text)\n",
    "    neg_score.append((row['url'], neg_count))\n",
    "    \n",
    "  for i in range(len(neg_score)):\n",
    "    for j in range(i + 1, len(neg_score)):\n",
    "      if neg_score[i][1] < neg_score[j][1]:\n",
    "        neg_score[i], neg_score[j] = neg_score[j], neg_score[i]\n",
    "  \n",
    "  return neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be749f3",
   "metadata": {},
   "source": [
    "The `RankByNeg` function calculates negative scores for each article and sorts the list of scores in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2831b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankByPos(text):\n",
    "  pos_score = []\n",
    "  \n",
    "  for index, row in text.iterrows():\n",
    "    text = processText(row['text'])\n",
    "    pos_count, neg_count = PosNeg(text)\n",
    "    pos_score.append((row['url'], pos_count))\n",
    "    \n",
    "  for i in range(len(pos_score)):\n",
    "    for j in range(i + 1, len(pos_score)):\n",
    "      if pos_score[i][1] < pos_score[j][1]:\n",
    "        pos_score[i], pos_score[j] = pos_score[j], pos_score[i]\n",
    "  \n",
    "  return pos_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18819d",
   "metadata": {},
   "source": [
    "The `RankByPos` function calculates positive scores for each article and sorts the list of scores in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ba3e0",
   "metadata": {},
   "source": [
    "This research is important because it brings focus to the important role of rhetoric in shaping climate change discussions. My findings present valuable information that can guide the development of more impactful communication strategies, inform media practices, and support advocacy efforts that are aimed at increasing climate action and policy.\n",
    "\n",
    "This study brings us one step closer to confirm that the way climate change is discussed significantly impacts public response. When language effectively conveys urgency and seriousness, it can promote meaningful action and create a deeper understanding of the crisis. On the other hand, language that downplays the severity or misrepresents facts can impede efforts to address climate change.\n",
    "\n",
    "how itâ€™s discussed in public discussion can either mobilize or hinder efforts in addressing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5e278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
